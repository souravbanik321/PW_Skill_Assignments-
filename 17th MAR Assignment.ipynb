{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ce113b-4aa6-438e-90a1-79f651052a1e",
   "metadata": {},
   "source": [
    "# Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc93f6-a06e-41e5-9661-02e4e3a519bd",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of a particular value in one or more observations in a variable. These missing values can occur for various reasons, such as data entry errors, non-response from survey participants, or data corruption during transmission. Missing values can affect the quality of data analysis and lead to biased results if not handled properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6779cbf5-2d2e-4bfc-8616-cb96620a8f43",
   "metadata": {},
   "source": [
    "It is essential to handle missing values in a dataset because they can lead to biased or inaccurate results in data analysis. Some of the reasons why missing values should be handled include:\n",
    "\n",
    "Missing values can reduce the power of statistical tests, making it difficult to detect significant relationships or differences.\n",
    "\n",
    "- They can affect the accuracy of predictive models, leading to poor performance in classification or regression tasks.\n",
    "\n",
    "-  They can cause problems with data visualization, making it difficult to see the full picture.\n",
    "\n",
    "-  Missing values can also affect the reliability and validity of research findings, leading to incorrect conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe7db3-c075-4cc8-8d4d-3dc65782a6c9",
   "metadata": {},
   "source": [
    "Some algorithms that are not affected by missing values include:Decision trees, Random forests, Support vector machines, K-nearest neighbors, Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a4c33-d4e3-43f7-a28d-7601098586dc",
   "metadata": {},
   "source": [
    "# Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021797f5-8fe6-4148-896a-fc6def6c68d3",
   "metadata": {},
   "source": [
    "`Mean/median imputation:`\n",
    "This technique involves filling in missing values with the mean or median of the existing values in the same variable. This is a simple technique that assumes the missing values are similar to the other values in the variable.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9718bb9c-0013-424d-8669-47c66e7ea584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   7.0\n",
      "2  3.0   8.0\n",
      "3  4.0   7.5\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4, 5], 'B': [6, 7, 8, np.nan, 10]})\n",
    "\n",
    "df['A'].fillna(df['A'].mean(), inplace=True)\n",
    "df['B'].fillna(df['B'].median(), inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7992c23-adea-4c6b-8fa6-b7e38a7e7774",
   "metadata": {},
   "source": [
    "`Mode imputation:`\n",
    "This technique involves filling in missing values with the mode (most common value) of the existing values in the same variable. This is suitable for categorical variables.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a57292-e011-4cd8-868c-3538eb332282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A      B\n",
      "0  cat    red\n",
      "1  dog  green\n",
      "2  cat    red\n",
      "3  cat    red\n",
      "4  dog   blue\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': ['cat', 'dog', np.nan, 'cat', 'dog'], 'B': ['red', 'green', 'red', np.nan, 'blue']})\n",
    "\n",
    "df['A'].fillna(df['A'].mode()[0], inplace=True)\n",
    "df['B'].fillna(df['B'].mode()[0], inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767301b-313f-4aae-8f35-b9cd40308130",
   "metadata": {},
   "source": [
    "`Deletion:`\n",
    "This technique involves deleting the rows or columns that contain missing values. This can be done if the missing data is relatively small compared to the overall dataset.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d23a73f-1829-41c8-b9c4-e19c615040c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   7.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4, 5], 'B': [6, 7, 8, np.nan, 10]})\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9516f9-daa9-4420-9e21-ba1e406e5377",
   "metadata": {},
   "source": [
    "`Interpolation:`\n",
    "This technique involves estimating the missing values based on the existing values in the same variable. This can be done using various methods such as linear interpolation, polynomial interpolation, or spline interpolation.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0636909e-87cb-4d5a-9c6e-9df6712ec297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   7.0\n",
      "2  3.0   8.0\n",
      "3  4.0   9.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4, 5], 'B': [6, 7, 8, np.nan, 10]})\n",
    "\n",
    "df.interpolate(method='linear', limit_direction='forward', inplace=True)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48345e06-76ad-446f-b27e-96ac75f73b08",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f36ce1a-7a20-4146-8e0e-f79f39596873",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation where the distribution of the target variable in a dataset is uneven or biased towards one of the classes. In other words, there are significantly more instances of one class than the other(s) in the dataset.\n",
    "\n",
    "For example, in a binary classification problem, if the target variable has 90% instances of one class and 10% instances of the other class, then it is an imbalanced dataset.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to several problems such as:\n",
    "\n",
    " 1.`Biased Model:` The resulting model will be biased towards the majority class, and it may classify all instances as belonging to the majority class, resulting in poor performance on the minority class.\n",
    "\n",
    " 2.`Poor Accuracy:` The accuracy metric will be misleading since it measures the overall accuracy of the model rather than the performance on each class. The model may have a high accuracy, but it will be poor on the minority class.\n",
    "\n",
    " 3. `Poor Generalization:` The model may perform well on the training data but poorly on new data, as it has not learned to differentiate between the classes.\n",
    "\n",
    "To overcome these problems, imbalanced data needs to be handled by applying appropriate techniques such as:\n",
    "\n",
    " 1.`Oversampling the minority` class to balance the class distribution.\n",
    "\n",
    " 2.`Undersampling the majority` class to balance the class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd43f8c-c31c-4014-96ab-1e9320cca4c3",
   "metadata": {},
   "source": [
    "# Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24ff8b4-a054-4bc5-846e-9d7099c022e2",
   "metadata": {},
   "source": [
    "Upsampling and downsampling are two techniques used to handle imbalanced data. They are used to balance the class distribution by increasing or decreasing the number of instances in a specific class.\n",
    "\n",
    "Upsampling involves increasing the number of instances in the minority class to balance the class distribution. This can be done by randomly duplicating the existing instances or by generating new instances using various techniques such as SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "Downsampling involves decreasing the number of instances in the majority class to balance the class distribution. This can be done by randomly removing instances from the majority class or by selecting a subset of instances from the majority class.\n",
    "\n",
    "Here is an example to illustrate when upsampling and downsampling are required:\n",
    "\n",
    "Suppose we have a dataset with a binary target variable indicating whether a customer will purchase a product or not. The dataset contains 1000 instances, out of which 900 instances belong to the negative class (customer will not purchase) and 100 instances belong to the positive class (customer will purchase).\n",
    "\n",
    "In this case, the dataset is imbalanced, and we need to balance the class distribution. Since the positive class is the minority class, we can use upsampling to increase the number of instances in the positive class. We can randomly duplicate the existing instances or use a technique like SMOTE to generate new instances.\n",
    "\n",
    "On the other hand, if the positive class had 10 instances and the negative class had 990 instances, we could use downsampling to decrease the number of instances in the negative class to balance the class distribution. We can randomly remove instances from the negative class or select a subset of instances.\n",
    "\n",
    "In summary, upsampling and downsampling are required when the class distribution is imbalanced, and we need to balance the distribution to avoid biased models and poor performance on the minority class. Upsampling is used when the positive class is the minority class, while downsampling is used when the positive class is the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d37e9a-599e-4c74-839c-7e76529dc09a",
   "metadata": {},
   "source": [
    "# Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec644071-2b8f-4ea6-a5d2-7b7bab4c3493",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to increase the size of a dataset by generating new instances from the existing instances. This is typically done by applying random transformations or perturbations to the existing instances, such as flipping, rotating, or cropping images or adding noise to signals.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a specific data augmentation technique used to address the problem of imbalanced datasets. SMOTE generates new synthetic instances for the minority class by interpolating between existing minority class instances.\n",
    "\n",
    "Here is a step-by-step explanation of SMOTE:\n",
    "\n",
    "    1.For each minority class instance, SMOTE selects k nearest neighbors from the minority class.\n",
    "\n",
    "    2.SMOTE then generates new synthetic instances by interpolating between the minority class instance and its k nearest neighbors.\n",
    "\n",
    "    2.The amount of interpolation is controlled by a parameter called the sampling ratio, which determines the number of synthetic instances to be generated.\n",
    "\n",
    "    4.SMOTE repeats this process for all minority class instances, resulting in a balanced dataset.\n",
    "\n",
    "For example, suppose we have a dataset with a binary target variable indicating whether a customer will churn or not. The dataset contains 1000 instances, out of which 900 instances belong to the negative class (customer will not churn) and 100 instances belong to the positive class (customer will churn). In this case, the dataset is imbalanced, and we can use SMOTE to generate new synthetic instances for the positive class.\n",
    "\n",
    "SMOTE will first select k nearest neighbors for each positive class instance, say k=5. It will then generate new synthetic instances by interpolating between the positive class instance and its 5 nearest neighbors. The amount of interpolation is controlled by the sampling ratio, say 0.5, which means that SMOTE will generate 50 new synthetic instances for the positive class.\n",
    "\n",
    "The resulting dataset will now have 900 instances for the negative class and 150 instances for the positive class, making it a balanced dataset. SMOTE helps to improve the performance of machine learning models on imbalanced datasets by providing additional information for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a2d3a-58d4-475e-95b2-2ec1ade26101",
   "metadata": {},
   "source": [
    "# Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206eaa40-6df4-4c22-9aa4-e2125db77e42",
   "metadata": {},
   "source": [
    "Outliers are data points that are significantly different from other data points in a dataset. They can be identified as extreme values that lie far away from the majority of the data points.\n",
    "\n",
    "It is essential to handle outliers for several reasons:\n",
    "\n",
    "    1.Outliers can distort the results of statistical analyses, such as mean and standard deviation, leading to biased and inaccurate results.\n",
    "\n",
    "    2.Outliers can also affect the performance of machine learning models, as they can have a significant impact on the estimates of the model parameters and can lead to poor generalization.\n",
    "\n",
    "    3.Outliers can also indicate errors in the data collection process, such as data entry errors or measurement errors, which need to be corrected.\n",
    "\n",
    "There are various techniques used to handle outliers in a dataset, including:\n",
    "\n",
    "1.`Removing the outliers:`  This involves removing the outliers from the dataset based on some predefined criteria, such as the interquartile range (IQR) or z-score. For example, we can remove any data points that lie outside of 1.5 times the IQR from the first and third quartiles of the data.\n",
    "\n",
    "2.`Transforming the data:` This involves transforming the data to make it more normally distributed, such as using a logarithmic or exponential transformation.\n",
    "\n",
    "3.`Binning the data:` This involves dividing the data into bins and treating each bin as a separate category.\n",
    "\n",
    "4.`Using robust statistical methods:` This involves using statistical methods that are less sensitive to outliers, such as the median instead of the mean or non-parametric methods.\n",
    "\n",
    "Handling outliers is essential to ensure the accuracy and reliability of statistical analyses and machine learning models. It helps to avoid biased and inaccurate results and ensures that the models are robust and generalizable to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b975669-e063-4a45-b600-63394df79f62",
   "metadata": {},
   "source": [
    "# Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2151a9-882f-4786-b2e8-c80525c13d68",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in customer data analysis. Some of the commonly used techniques are:\n",
    "\n",
    "`Deleting the missing data:` This involves deleting the rows or columns that contain missing data. However, this technique is only recommended if the missing data is small and does not significantly affect the analysis.\n",
    "\n",
    "`Mean/median imputation:` This involves replacing the missing values with the mean or median value of the variable. This technique is useful for numerical data and assumes that the missing values are randomly distributed.\n",
    "\n",
    "`Mode imputation:` This involves replacing the missing values with the mode (most frequent value) of the variable. This technique is useful for categorical data and assumes that the missing values are randomly distributed.\n",
    "\n",
    "`Regression imputation:` This involves using regression models to predict the missing values based on the values of other variables in the dataset. This technique is useful when the missing values are not randomly distributed and are related to other variables in the dataset.\n",
    "\n",
    "`Multiple imputation:` This involves creating multiple imputed datasets based on the existing data and using statistical techniques to combine the results from these datasets. This technique is useful when the missing values are not completely at random and are related to other variables in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d329aef2-6539-4fe6-9874-0ff4848b251c",
   "metadata": {},
   "source": [
    "# Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d276961c-e76b-43c7-93b6-41d6574d877d",
   "metadata": {},
   "source": [
    "There are several strategies that can be used to determine if the missing data is missing at random (MAR) or if there is a pattern to the missing data. Some of these strategies are:\n",
    "\n",
    "1. `Visual inspection:` One of the simplest strategies is to visualize the missing data using graphs, such as histograms, boxplots, or heatmaps, and look for any patterns or correlations between the missing data and other variables.\n",
    "\n",
    "2. `Correlation analysis:` Another strategy is to calculate the correlation coefficients between the missing data and other variables in the dataset. If there is no significant correlation, it may suggest that the missing data is MAR.\n",
    "\n",
    "3. `Imputation methods:` Imputation methods can also provide insights into the missing data patterns. If the imputed values are similar to the observed values, it may suggest that the missing data is MAR. On the other hand, if the imputed values are significantly different, it may suggest that the missing data is not MAR.\n",
    "\n",
    "4. `Statistical tests:` Statistical tests, such as the Little's MCAR test or the pattern-mixture models, can also be used to test the missing data patterns. These tests can provide information on whether the missing data is MAR or not.\n",
    "\n",
    "5. `Domain knowledge:` Finally, domain knowledge can also provide insights into the missing data patterns. If there is a logical reason why the data is missing, such as a survey question that was not answered, it may suggest that the missing data is not MAR.\n",
    "\n",
    "It is important to note that these strategies are not mutually exclusive, and a combination of these strategies can provide more robust insights into the missing data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378b5cd-67de-4846-ad87-12196ff4bbac",
   "metadata": {},
   "source": [
    "# Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcdf305-229f-4621-bf1a-890136ffdeb1",
   "metadata": {},
   "source": [
    "When working with imbalanced datasets, where one class is significantly underrepresented compared to the other, the accuracy of a machine learning model can be misleading. Therefore, it is essential to use appropriate evaluation metrics and strategies to assess the performance of the model. Some of the strategies that can be used to evaluate the performance of a machine learning model on an imbalanced dataset are:\n",
    "\n",
    "1. `Confusion matrix:` A confusion matrix is a table that summarizes the classification results of a model. It can be used to calculate various evaluation metrics, such as precision, recall, and F1-score, for both the minority and majority classes.\n",
    "\n",
    "2. `Resampling techniques:` Resampling techniques, such as oversampling and undersampling, can be used to balance the dataset. Oversampling involves creating more samples of the minority class, while undersampling involves removing samples from the majority class. This can help the model to learn from both classes equally and improve its performance on the minority class.\n",
    "\n",
    "3. `Cost-sensitive learning:` Cost-sensitive learning involves adjusting the misclassification costs for the minority and majority classes. This can help the model to prioritize the correct classification of the minority class, which is more critical in a medical diagnosis project.\n",
    "\n",
    "4. `Ensembling methods:` Ensembling methods, such as bagging and boosting, can be used to combine the predictions of multiple models to improve the overall performance. For example, boosting can be used to give more weight to misclassified samples of the minority class, while bagging can be used to reduce overfitting on the majority class.\n",
    "\n",
    "5. `Using appropriate evaluation metrics:` Accuracy is not an appropriate evaluation metric for imbalanced datasets. Instead, evaluation metrics such as precision, recall, F1-score, area under the ROC curve (AUC-ROC), and area under the precision-recall curve (AUC-PR) can provide more accurate information on the model's performance on the minority and majority classes.\n",
    "\n",
    "It is important to note that the choice of strategy will depend on the specific project requirements and the nature of the imbalanced dataset. Therefore, it is essential to evaluate multiple strategies and choose the one that provides the best results for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9bfe51-f9cf-49c2-bf8c-2b9bd2c821a0",
   "metadata": {},
   "source": [
    "# Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c793ad-929a-4a9f-b940-97b396fa3935",
   "metadata": {},
   "source": [
    "When dealing with an unbalanced dataset, where the majority class is overrepresented compared to the minority class, down-sampling can be used to balance the dataset. Down-sampling involves randomly removing samples from the majority class to match the number of samples in the minority class. This can help the model to learn from both classes equally and improve its performance on the minority class.\n",
    "\n",
    "To down-sample the majority class in Python, the following steps can be followed:\n",
    "\n",
    "1. Separate the majority and minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21a023-c33b-4931-b681-3bdf6559145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_class = df[df['satisfaction'] == 'satisfied']\n",
    "minority_class = df[df['satisfaction'] == 'unsatisfied']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0215487-23c6-4f95-a4fa-108b7949b0c6",
   "metadata": {},
   "source": [
    "2. Determine the number of samples in the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87a270-f455-47d2-a4a0-17b8ca143121",
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_class_size = len(minority_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dbc0ac-d29e-4832-9e72-49c4c800e7c3",
   "metadata": {},
   "source": [
    "3. Sample a random subset of the majority class equal to the number of samples in the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f65e5-03af-491b-9d03-8eff04f294db",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_class_downsampled = majority_class.sample(n=minority_class_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e6d9c-7e4a-48b4-9931-dfad1dc563fe",
   "metadata": {},
   "source": [
    "4. Combine the minority class and down-sampled majority class to create the balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7606b1e0-26f0-4339-baac-2cb21f27a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = pd.concat([minority_class, majority_class_downsampled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd8a52c-19a1-44b0-bf62-a13c4f32cb54",
   "metadata": {},
   "source": [
    "In addition to down-sampling, other techniques such as oversampling, SMOTE, or using appropriate evaluation metrics can also be used to handle imbalanced datasets. The choice of technique will depend on the specific project requirements and the nature of the imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba093ee-7826-480d-838c-ac57291e1af2",
   "metadata": {},
   "source": [
    "# Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b203a1d-23e1-479c-8bb4-851f2a6e21e9",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset where the minority class is underrepresented, up-sampling can be used to balance the dataset. Up-sampling involves randomly duplicating samples from the minority class to increase their number and make them equal to the majority class. This can help the model to learn from both classes equally and improve its performance on the minority class.\n",
    "\n",
    "To up-sample the minority class in Python, the following steps can be followed:\n",
    "\n",
    "1. Separate the majority and minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882586c-7b78-4941-8014-fb0e67477aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_class = df[df['target'] == 0]\n",
    "minority_class = df[df['target'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16900a49-cd21-4d79-9df0-a75b7617ffbc",
   "metadata": {},
   "source": [
    "2. Determine the number of samples in the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1b4e8-b525-4d22-9830-af55fcf40ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_class_size = len(majority_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f82163-7e2e-4551-9d2d-b7d71fa62040",
   "metadata": {},
   "source": [
    "3. Resample the minority class with replacement to match the number of samples in the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d6190-9e85-49e5-8bb6-816a8708f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_class_upsampled = minority_class.sample(n=majority_class_size, replace=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f76cc0-759c-40fa-acdb-a141b6338e51",
   "metadata": {},
   "source": [
    "4. Combine the minority class and up-sampled majority class to create the balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821fdcb-e73f-4d42-86dd-d88c41505cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = pd.concat([minority_class_upsampled, majority_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c0804-be30-4442-aab5-35bda311e6be",
   "metadata": {},
   "source": [
    "In addition to up-sampling, other techniques such as down-sampling, SMOTE, or using appropriate evaluation metrics can also be used to handle imbalanced datasets. The choice of technique will depend on the specific project requirements and the nature of the imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b463e78-4764-4ec8-a1a0-4aa030b1ea8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
